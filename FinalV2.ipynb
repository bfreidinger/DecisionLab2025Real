{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7HRut0G10Jpz"
      },
      "source": [
        "## Price Data Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46llzqZnp3UZ",
        "outputId": "4ec2a311-3ca2-4469-e4fe-3fc78ea45c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# -----------------------\n",
        "# 1) Download price data \n",
        "# -----------------------\n",
        "tickers = [\n",
        "    \"AAPL\", \"MSFT\", \"AMZN\", \"NVDA\",\n",
        "    \"TSLA\", \"GOOG\", \"BRK-B\", \"META\", \"UNH\", \"SPY\"\n",
        "]\n",
        "\n",
        "end_date = datetime.datetime.today()\n",
        "start_date = end_date - datetime.timedelta(days=12*365)\n",
        "\n",
        "data = yf.download(tickers, start=start_date, end=end_date, progress=False)\n",
        "\n",
        "if 'Adj Close' in data.columns.get_level_values(0):\n",
        "    adj_close = data['Adj Close'].copy()\n",
        "else:\n",
        "    adj_close = data['Close'].copy()\n",
        "\n",
        "if 'BRK-B' not in adj_close.columns and 'BRK.B' in adj_close.columns:\n",
        "    adj_close.rename(columns={'BRK.B': 'BRK-B'}, inplace=True)\n",
        "\n",
        "adj_close.dropna(how='all', inplace=True)\n",
        "\n",
        "# -----------------------\n",
        "# 2) Compute daily returns\n",
        "# -----------------------\n",
        "daily_returns = np.log(adj_close).diff().dropna()\n",
        "eq_tickers = [t for t in tickers if t != \"SPY\"]\n",
        "returns_for_pca = daily_returns[eq_tickers].copy()\n",
        "# -----------------------\n",
        "# 3) PCA on daily returns\n",
        "# -----------------------\n",
        "pca_model = PCA(n_components=2)\n",
        "pca_factors = pca_model.fit_transform(returns_for_pca.fillna(0))\n",
        "loadings = pca_model.components_\n",
        "\n",
        "factors_df = pd.DataFrame(\n",
        "    pca_factors,\n",
        "    index=returns_for_pca.index,\n",
        "    columns=[f\"Factor{i+1}\" for i in range(2)]\n",
        ")\n",
        "\n",
        "recon_returns = pca_factors @ loadings\n",
        "recon_df = pd.DataFrame(recon_returns, index=returns_for_pca.index, columns=eq_tickers)\n",
        "idio_returns = returns_for_pca - recon_df\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Convert everything to weekly returns\n",
        "# -------------------------------\n",
        "weekly_returns = daily_returns.resample(\"W-MON\").sum()\n",
        "weekly_idio_returns = idio_returns.resample(\"W-MON\").sum()\n",
        "weekly_factors_df = factors_df.resample(\"W-MON\").sum()\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Create weekly signal targets\n",
        "# -------------------------------\n",
        "future_weekly = weekly_idio_returns.shift(-1)\n",
        "future_weekly_factor2 = weekly_factors_df[\"Factor2\"].shift(-1)\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Create weekly predictive signals\n",
        "# -------------------------------\n",
        "def make_correlated_signal(target, desired_corr=0.2, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    rnd_noise = np.random.randn(len(target))\n",
        "    t_std = (target - target.mean()) / (target.std() + 1e-8)\n",
        "    r_std = (rnd_noise - rnd_noise.mean()) / (rnd_noise.std() + 1e-8)\n",
        "    alpha = desired_corr\n",
        "    beta = np.sqrt(1 - alpha**2)\n",
        "    signal = alpha * t_std + beta * r_std\n",
        "    signal = signal * target.std() + target.mean()\n",
        "    return pd.Series(signal, index=target.index)\n",
        "\n",
        "weekly_stock_signals = {}\n",
        "for stock in eq_tickers:\n",
        "    target = future_weekly[stock].dropna()\n",
        "    sig = make_correlated_signal(target, desired_corr=0.1, seed=42)\n",
        "    weekly_stock_signals[stock] = sig.reindex(future_weekly.index)\n",
        "\n",
        "weekly_stock_signals_df = pd.DataFrame(weekly_stock_signals)\n",
        "\n",
        "factor2_signal = make_correlated_signal(\n",
        "    future_weekly_factor2.dropna(), desired_corr=0.3, seed=42\n",
        ").reindex(future_weekly_factor2.index)\n",
        "\n",
        "weekly_factor_signals_df = pd.DataFrame({\n",
        "    \"Factor2_signal\": factor2_signal\n",
        "})\n",
        "\n",
        "# -------------------------------------\n",
        "# 7) Combine weekly data and apply noise mask\n",
        "# -------------------------------------\n",
        "combined_data = pd.concat([\n",
        "    weekly_returns,\n",
        "    # weekly_idio_returns.add_prefix(\"IDIO_\"),\n",
        "    # weekly_factors_df.add_prefix(\"FCT_\"),\n",
        "], axis=1)\n",
        "\n",
        "combined_data_missing = combined_data.copy()\n",
        "can_have_nan_cols = [col for col in combined_data.columns if \"SPY\" not in col]\n",
        "\n",
        "np.random.seed(999)\n",
        "mask_size = int(0.01 * combined_data_missing[can_have_nan_cols].size)\n",
        "row_indices = np.random.choice(combined_data_missing.index, mask_size)\n",
        "col_indices = np.random.choice(can_have_nan_cols, mask_size)\n",
        "combined_data_missing.values[\n",
        "    tuple(zip(*[\n",
        "        (combined_data_missing.index.get_loc(r), combined_data_missing.columns.get_loc(c))\n",
        "        for r, c in zip(row_indices, col_indices)\n",
        "    ]))\n",
        "] = np.nan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yt6iIJyxqYWa"
      },
      "outputs": [],
      "source": [
        "X = combined_data[eq_tickers].to_numpy().T\n",
        "\n",
        "B = loadings.T\n",
        "\n",
        "N, T = X.shape\n",
        "\n",
        "index_train = range(int(0.75 * T))\n",
        "index_test = range(int(0.75 * T), T)\n",
        "\n",
        "X_train = X[:, index_train]\n",
        "X_test = X[:, index_test]\n",
        "\n",
        "T_train, T_test = X_train.shape[1], X_test.shape[1]\n",
        "\n",
        "timestamps = combined_data.index\n",
        "timestamps_train = timestamps[index_train]\n",
        "timestamps_test = timestamps[index_test]\n",
        "\n",
        "signals = weekly_stock_signals_df.to_numpy().T\n",
        "signals[:, signals.shape[1] - 1] = 0\n",
        "signals_train = signals[:, index_train]\n",
        "signals_test = signals[:, index_test]\n",
        "\n",
        "factor_signal = weekly_factor_signals_df.to_numpy().T\n",
        "factor_signal[:, factor_signal.shape[1] - 1] = 0\n",
        "factor_signal_train = factor_signal[:, index_train]\n",
        "factor_signal_test = factor_signal[:, index_test]\n",
        "\n",
        "X_idio = weekly_idio_returns[eq_tickers].to_numpy().T\n",
        "X_idio_train = X_idio[:, index_train]\n",
        "X_idio_test = X_idio[:, index_test]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-4epal57ygfT"
      },
      "source": [
        "## Rolling Correlations And Volatilities"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q7m64rKVqy7D"
      },
      "source": [
        "Given NxT matrix X, give a NxT matrix which is the rolling std of each asset over the past 20 days. Such that column i is made using data from [i-20, i-1].\n",
        "\n",
        "1.   Should we de-mean? Yes\n",
        "2.   What shape should rolling_std_matrix be? NxT, with first 20 columns = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0vHV8Mlvqovk"
      },
      "outputs": [],
      "source": [
        "std_window = 20\n",
        "rolling_std_matrix = np.full((N, T), np.nan)\n",
        "for i in range(N):\n",
        "    current_row = pd.Series(X[i])\n",
        "    undemeaned = current_row.rolling(window=std_window).std()\n",
        "    rolling_std_matrix[i] = undemeaned - undemeaned.mean()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0hxmjqCSrku8"
      },
      "source": [
        "Find a TxNxN matrix from matrix X which is the rolling correlations over the past 50 days. Such that index i is made using data from [i-50, i-1].\n",
        "\n",
        "1.   Should we use de-meaned returns? Yes\n",
        "2.   What shape should rolling_corr_matrix be? TxNxN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J1-JuCvGq3TR"
      },
      "outputs": [],
      "source": [
        "corr_window = 50\n",
        "rolling_corr_matrix = np.full((T, N, N), np.nan)\n",
        "demeaned_returns = X.T - X.T.mean()\n",
        "for t in range(corr_window-1,T):\n",
        "    current_data = pd.DataFrame(demeaned_returns[t - corr_window + 1 : t + 1])\n",
        "    correl_matrix = current_data.corr().values\n",
        "    rolling_corr_matrix[t] = correl_matrix\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uQwjdQuMzLTF"
      },
      "source": [
        "## Generating mu_{t+1} on training data, and predicting on test data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vgbIlhrps4Ua"
      },
      "source": [
        "Explanation of Signals Matrix\n",
        "1.   signals is a NxT np.ndarray where signals[i, j] predicts the return of stock i over [j, j+1]. It is meant to predict X_idio[i, j+1]. Why then would we run regressions on X_idio[i, j+1] = A_i + B_i * signals[i, j]?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EG-R4aCIsz7i"
      },
      "source": [
        "Run Linear Regressions On X_idio_train For Each Stock to Find A_i and B_i for i = 1, ... , N\n",
        "\n",
        "1.   Generate intercepts, betas, each of length N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "a1vISwppsSWT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.13163126 0.15301576 0.153148   0.07681804 0.05061544 0.0508469\n",
            " 0.05332935 0.10590588 0.03877163] [0.00378901 0.00355955 0.00357084 0.00849852 0.00643533 0.00308245\n",
            " 0.00240446 0.00418856 0.00330191]\n"
          ]
        }
      ],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "intercepts = np.zeros(N)\n",
        "betas = np.zeros(N)\n",
        "for i in range(N):\n",
        "    x = signals[i, :-1]\n",
        "    Y = X_idio[i, 1:]\n",
        "    x_with_const = sm.add_constant(x)\n",
        "    model = sm.OLS(Y, x_with_const).fit()\n",
        "    intercepts[i] = model.params[0]\n",
        "    betas[i] = model.params[1]\n",
        "print(betas, intercepts)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ldqf_Mtt_L"
      },
      "source": [
        "Generate mu_test, which will be our means vector, , which is the forecast of r_{t+1}\n",
        "\n",
        "1.   What shape is mu_test? NxT_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5eEIN_V7vA4j"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (797304896.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    mu_test[:, 1:] =  # Includes signal\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "mu_test = np.zeros((N, T_test))\n",
        "\n",
        "mu_test[:, 1:] =   # Includes signal\n",
        "mu_test[:, 0] =   # Does not have signal, what does this mean?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MsemnDGGRYtt"
      },
      "source": [
        "### Repeating This on Factor Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfz28y11Rd9c"
      },
      "outputs": [],
      "source": [
        "# CODE HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zBhZQs-uzUIC"
      },
      "source": [
        "## Basic Max-Sharpe Portfolio Optimization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wrEZdDFPvCxy"
      },
      "source": [
        "Okay, for now we are not going to use MOSEK. Suppose that to have an optimal Sharpe portfolio, we use the formula below. (Don't Run This Block)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej9Tm8p3vqgL"
      },
      "outputs": [],
      "source": [
        "# Sigma = np.diag(stds) @ corr @ np.diag(stds)\n",
        "# weights = (np.linalg.inv(Sigma).dot(mu)) / np.ones(N).dot(np.linalg.inv(Sigma).dot(mu))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Alh1smWczcdC"
      },
      "source": [
        "Now, generate a N x T_train OOS matrix of standard deviations. Remember we had a N x T matrix. How do we subset it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5XllKa33n1a"
      },
      "outputs": [],
      "source": [
        "stds_matrix_oos ="
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qQe_MwTZ3uxx"
      },
      "source": [
        "Now, generate a T_train x N x N OOS matrix of correlations. Remember we had a T x N x N matrix. How do we subset it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uau9dtoI32VJ"
      },
      "outputs": [],
      "source": [
        "corr_matrix_oos ="
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bDEcfWEuwRL6"
      },
      "source": [
        "Examine standard deviations, correlation, means vector, and the weights. Why do we have the sum of weights = 1? Also, why do the weights appear this way?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm8kK4Lbwqq0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "corr_now = corr_matrix_oos[0]\n",
        "stds_now = stds_matrix_oos[:, 0]\n",
        "mu_now = mu_test[:, 0]\n",
        "# IF YOU ADDED THE FACTOR SIGNAL, CORRECT THE MU_NOW HERE\n",
        "mu_now = .....\n",
        "\n",
        "\n",
        "Sigma_now =\n",
        "weights_now =\n",
        "\n",
        "corr_df = pd.DataFrame(corr_now)\n",
        "corr_df.columns = eq_tickers\n",
        "corr_df.index = eq_tickers\n",
        "\n",
        "corr_df_rounded = corr_df.round(2)\n",
        "\n",
        "stds_df = pd.DataFrame(np.round(stds_now, 4))\n",
        "stds_df.index = eq_tickers\n",
        "\n",
        "mu_df = pd.DataFrame(np.round(mu_now, 4))\n",
        "mu_df.index = eq_tickers\n",
        "\n",
        "weights_df = pd.DataFrame(np.round(weights_now, 4))\n",
        "weights_df.index = eq_tickers\n",
        "\n",
        "print(\"Correlation Matrix\")\n",
        "sns.heatmap(corr_df_rounded, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Expected Returns\")\n",
        "display(mu_df)\n",
        "print(\"Daily Standard Deviations\")\n",
        "display(stds_df)\n",
        "print(\"Portfolio Weights\")\n",
        "display(weights_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YE9vkZSez1f6"
      },
      "source": [
        "## MOSEK Portfolio Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hpx0jP2ks9G"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "pip install mosek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb2psip5kaFn",
        "outputId": "eadcff05-1891-4228-8625-929063023f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.environ['MOSEKLM_LICENSE_FILE'] = '/content/drive/MyDrive/mosek/mosek.lic'\n",
        "# MAKE SURE MOSEK.LIC file is inside a mosek folder in your google drive\n",
        "\n",
        "import mosek\n",
        "from mosek.fusion import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bAAhor6L5upJ"
      },
      "source": [
        "### Basic Mean-Variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iyj1F-Qj8jg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mosek.fusion import Model, Expr, Domain, ObjectiveSense, Matrix\n",
        "\n",
        "portfolio_weights_df = pd.DataFrame(columns = eq_tickers + ['timestamp'])\n",
        "\n",
        "for i in range(T_test - 1):\n",
        "\n",
        "    corr_now = corr_matrix_oos[i]\n",
        "    stds_now = stds_matrix_oos[:, i]\n",
        "    mu_now = mu_test[:, i + 1]\n",
        "\n",
        "    # IF YOU ADDED THE FACTOR SIGNAL, CORRECT THE MU_NOW HERE\n",
        "    # mu_now = .....\n",
        "\n",
        "    Sigma_now = np.diag(stds_now) @ corr_now @ np.diag(stds_now)\n",
        "\n",
        "    S = Sigma_now\n",
        "    m = mu_now\n",
        "\n",
        "    N = m.shape[0]\n",
        "\n",
        "    # Weekly Vol Constraint at 0.1\n",
        "    gamma2 = 0.1 ** 2\n",
        "\n",
        "    # Compute the Cholesky factor of S\n",
        "    G = np.linalg.cholesky(S)\n",
        "    G_mat = Matrix.dense(G)  # shape NxN\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Basic MarkowitzReturn Model\n",
        "    # --------------------------------------------------\n",
        "    with Model(\"MarkowitzReturn_Basic\") as M:\n",
        "        # 1) Variables: fraction of holdings in each security (no short-selling)\n",
        "        # x = M.variable(\"x\", N, Domain.greaterThan(0.0))\n",
        "        x = M.variable(\"x\", N)\n",
        "\n",
        "        # 2) Budget constraint: sum(x) == 1\n",
        "        M.constraint(\"budget\", Expr.sum(x), Domain.equalsTo(1.0))\n",
        "\n",
        "        # 3) Objective: maximize expected return\n",
        "        M.objective(\"obj\", ObjectiveSense.Maximize, Expr.dot(x, m))\n",
        "\n",
        "        # 4) Risk constraint using rotated QCone:\n",
        "        #    vstack( gamma2, 0.5, G^T x ) in RQCone\n",
        "        G_matT = Matrix.dense(G.T)  # NxN\n",
        "        M.constraint(\n",
        "            \"risk\",\n",
        "            Expr.vstack(gamma2, 0.5, Expr.mul(G_matT, x)),\n",
        "            Domain.inRotatedQCone()\n",
        "        )\n",
        "\n",
        "        # Solve\n",
        "        M.solve()\n",
        "        returns = M.primalObjValue()\n",
        "        portfolio = x.level()\n",
        "\n",
        "    portfolio = portfolio / sum(abs(portfolio))\n",
        "    portfolio_weights_df.loc[i] = list(portfolio) + [timestamps_test[i]]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "thi1nWle56oO"
      },
      "source": [
        "### Constrained Mean-Variance\n",
        "\n",
        "\n",
        "In the code, we see the lines\n",
        "        if USE_MARKET_NEUTRAL:\n",
        "            # factor^T x = 0\n",
        "            M.constraint(\"market_neutral\",\n",
        "                         Expr.dot(factor, x),\n",
        "                         Domain.equalsTo(0.0))\n",
        "\n",
        "* Play around with this. If you change the level of market exposure, and include the market signal.  What does it do for the future returns?\n",
        "\n",
        "* Can you plot future returns against the factor returns, and show your market factor portfolio weights across time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8v8UhHPt3VI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mosek.fusion import Model, Expr, Domain, ObjectiveSense, Matrix\n",
        "\n",
        "constrained_portfolio_weights_df = pd.DataFrame(columns = eq_tickers + ['timestamp'])\n",
        "factor = loadings[0, :]\n",
        "# User-defined toggles\n",
        "USE_MARKET_NEUTRAL = True\n",
        "USE_TRANSACTION_COSTS = True\n",
        "USE_LEVERAGE = True\n",
        "\n",
        "turnover_limit = 1e9\n",
        "cost_rate = 0.0002\n",
        "leverage_limit = 2.0\n",
        "gamma2 = 0.1 ** 2\n",
        "\n",
        "x0 = np.array(portfolio_weights_df.loc[0, eq_tickers].values, dtype=float)\n",
        "\n",
        "for i in range(T_test - 1):\n",
        "\n",
        "    corr_now = corr_matrix_oos[i]\n",
        "    stds_now = stds_matrix_oos[:, i]\n",
        "    mu_now = mu_test[:, i + 1]\n",
        "\n",
        "    # IF YOU ADDED THE FACTOR SIGNAL, CORRECT THE MU_NOW HERE\n",
        "    # mu_now = .....\n",
        "\n",
        "    Sigma_now = np.diag(stds_now) @ corr_now @ np.diag(stds_now)\n",
        "\n",
        "    S = Sigma_now\n",
        "    m = mu_now\n",
        "    N = m.shape[0]\n",
        "\n",
        "    # Cholesky factor of S for the risk constraint\n",
        "    G = np.linalg.cholesky(S)\n",
        "    G_mat = Matrix.dense(G)\n",
        "\n",
        "    # --------------------------------------------------\n",
        "    # Extended MarkowitzReturn Model\n",
        "    # --------------------------------------------------\n",
        "    with Model(\"MarkowitzReturn_Extended\") as M:\n",
        "\n",
        "        # 1) Decision variables\n",
        "        # x can be unbounded if we allow shorting\n",
        "        x = M.variable(\"x\", N, Domain.unbounded())\n",
        "\n",
        "        # (a) For turnover, define d+ and d- to measure trades vs x0\n",
        "        d_plus  = M.variable(\"d_plus\",  N, Domain.greaterThan(0.0))\n",
        "        d_minus = M.variable(\"d_minus\", N, Domain.greaterThan(0.0))\n",
        "\n",
        "        # (b) For leverage, define z to measure absolute value of x\n",
        "        z = M.variable(\"z\", N, Domain.greaterThan(0.0))\n",
        "\n",
        "        # 2) Constraints\n",
        "\n",
        "        # 2a) Market Neutral (optional)\n",
        "        if USE_MARKET_NEUTRAL:\n",
        "            # factor^T x = 0\n",
        "            M.constraint(\"market_neutral\",\n",
        "                         Expr.dot(factor, x),\n",
        "                         Domain.equalsTo(0.0))\n",
        "\n",
        "\n",
        "        # 2c) Leverage (optional)\n",
        "        # sum(|x|) <= leverage_limit\n",
        "        if USE_LEVERAGE:\n",
        "            # z_i >= x_i and z_i >= -x_i\n",
        "            M.constraint(\"z_gte_posX\", Expr.sub(x, z), Domain.lessThan(0.0))\n",
        "            M.constraint(\"z_gte_negX\", Expr.add(x, z), Domain.greaterThan(0.0))\n",
        "            # sum(z_i) <= c\n",
        "            M.constraint(\"leverage\", Expr.sum(z), Domain.lessThan(leverage_limit))\n",
        "\n",
        "        # 2e) Risk constraint: x^T S x <= gamma2\n",
        "        # Rotated QCone form: vstack(gamma2, 0.5, G^T x) in RQCone\n",
        "        G_matT = Matrix.dense(G.T)\n",
        "        M.constraint(\n",
        "            \"risk\",\n",
        "            Expr.vstack(gamma2, 0.5, Expr.mul(G_matT, x)),\n",
        "            Domain.inRotatedQCone()\n",
        "        )\n",
        "\n",
        "        # Turnover limit\n",
        "        M.constraint(\"turnover_link\", Expr.sub(Expr.sub(x, x0), Expr.sub(d_plus, d_minus)), Domain.equalsTo(0.0))\n",
        "        M.constraint(\"turnover_limit\", Expr.sum(Expr.add(d_plus, d_minus)), Domain.lessThan(turnover_limit))\n",
        "\n",
        "        # 3) Objective\n",
        "        # Gross return: dot(m, x)\n",
        "        # Transaction cost (optional): cost_rate * sum(d+ + d-)\n",
        "        if USE_TRANSACTION_COSTS:\n",
        "            tc_expr = Expr.mul(cost_rate,\n",
        "                               Expr.add(Expr.sum(d_plus), Expr.sum(d_minus)))\n",
        "            net_return = Expr.sub(Expr.dot(m, x), tc_expr)\n",
        "        else:\n",
        "            net_return = Expr.dot(m, x)\n",
        "\n",
        "        M.objective(\"obj\", ObjectiveSense.Maximize, net_return)\n",
        "\n",
        "        # 4) Solve\n",
        "\n",
        "        try:\n",
        "            M.solve()\n",
        "            returns = M.primalObjValue()\n",
        "            portfolio = x.level()\n",
        "            x0 = portfolio\n",
        "        except:\n",
        "            portfolio = x0\n",
        "\n",
        "\n",
        "    constrained_portfolio_weights_df.loc[i] = list(portfolio) + [timestamps_test[i]]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HWOeZ9Rz69j2"
      },
      "source": [
        "### Examining Portfolio Weights Over Time (Two MOSEK Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0RRaXqeptTf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the portfolio weights over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot Portfolio Weights Here\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Portfolio Weight')\n",
        "plt.title('Portfolio Weights Over Time')\n",
        "plt.legend(title=\"Assets\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URND4F_p7Cam"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the portfolio weights over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot Constrained Portfolio Weights Here\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Portfolio Weight')\n",
        "plt.title('Constrained Portfolio Weights Over Time')\n",
        "plt.legend(title=\"Assets\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3jEUvOjE_P27"
      },
      "source": [
        "### Plot Portfolio Returns Over Time for Both Portfolios"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmnb_44PfGwv"
      },
      "source": [
        "1.   Portfolio Weights (T_oos - 1) x N\n",
        "2.   Portfolio Return (T_oos - 1) x 1\n",
        "3.   Shifted returns from t=1 to t=T_oos-1 (T_oos - 1) x 1\n",
        "* Why shifted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2H6AL36q2Nh"
      },
      "outputs": [],
      "source": [
        "# CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "# Now you can store portfolio_returns and portfolio_timestamps, and plot them\n",
        "portfolio_returns_df = pd.DataFrame({\n",
        "    'timestamp': portfolio_timestamps,\n",
        "    'portfolio_return': portfolio_returns,\n",
        "    'constrained_portfolio_return': constrained_portfolio_returns\n",
        "})\n",
        "\n",
        "# Plot the portfolio returns over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(portfolio_returns_df['timestamp'], portfolio_returns_df['portfolio_return'].cumsum(), label='Portfolio Return')\n",
        "plt.plot(portfolio_returns_df['timestamp'], portfolio_returns_df['constrained_portfolio_return'].cumsum(), label='Constrained Portfolio Return')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Return')\n",
        "plt.title('Returns Over Time')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tqdRoiRzzy36"
      },
      "source": [
        "Generating Market Exposure From Weights\n",
        "\n",
        "1.  Dot(weights, factor) -> Dot(TxN, Nx1) -> Nx1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Smch50bRSUu"
      },
      "outputs": [],
      "source": [
        "second_factor = loadings[1, :]\n",
        "\n",
        "\n",
        "market_exposure_df = pd.DataFrame({\n",
        "    'portfolio_market_exposure': ....,\n",
        "    'constrained_portfolio_market_exposure': ....,\n",
        "      'portfolio_second_factor_exposure': ....,\n",
        "    'constrained_portfolio_second_factor_exposure': ....,\n",
        "}, index = portfolio_timestamps)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WU-pCCubyMfS"
      },
      "source": [
        "Plotting Market Exposure Over Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uBdfX4TyPSi"
      },
      "outputs": [],
      "source": [
        "plt.plot(market_exposure_df['portfolio_market_exposure'], label = 'Mean-Variance Portfolio')\n",
        "plt.plot(market_exposure_df['constrained_portfolio_market_exposure'], label = 'Constrained Mean-Variance Portfolio')\n",
        "plt.title(\"Market Exposure Over Time\")\n",
        "plt.xlabel(\"Week\")\n",
        "plt.ylabel(\"Exposure\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(market_exposure_df['portfolio_second_factor_exposure'], label = 'Mean-Variance Portfolio')\n",
        "plt.plot(market_exposure_df['constrained_portfolio_second_factor_exposure'], label = 'Constrained Mean-Variance Portfolio')\n",
        "plt.title(\"Factor 2 Exposure Over Time\")\n",
        "plt.xlabel(\"Week\")\n",
        "plt.ylabel(\"Exposure\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V-OHfBj0QizG"
      },
      "source": [
        "## Additional Questions\n",
        "\n",
        "* Change the level of market exposure, and include / don't include the market signal.  What does it do for returns?\n",
        "\n",
        "* Can you plot portfolio returns against the factor returns, and show your market factor weight across time?\n",
        "\n",
        "* Do you get short the market when it goes down? Plot market factor weight vs. market factor return? Should be some > 0 correlation.\n",
        "\n",
        "* What does the second factor seem to mean? Play around with the code in the first block, and make a negatively correlated signal if you want.\n",
        "\n",
        "* Alter the constraints. Use ChatGPT, and make plots."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
